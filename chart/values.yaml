# Default Values primarily pulled from https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml

cephBlockPool:
  enabled: true
  # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
  spec:
    failureDomain: host
    replicated:
      size: ###ZARF_VAR_HA_REPLICAS###
    # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
    # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
    # enableRBDStats: true
  storageClass:
    isDefault: true
    reclaimPolicy: Retain
    allowVolumeExpansion: true
    volumeBindingMode: "Immediate"
    mountOptions: []
    # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
    allowedTopologies: []
    #        - matchLabelExpressions:
    #            - key: rook-ceph-role
    #              values:
    #                - storage-node
    # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
    parameters:
      # (optional) mapOptions is a comma-separated list of map options.
      # For krbd options refer
      # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
      # For nbd options refer
      # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
      # mapOptions: lock_on_read,queue_depth=1024

      # (optional) unmapOptions is a comma-separated list of unmap options.
      # For krbd options refer
      # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
      # For nbd options refer
      # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
      # unmapOptions: force

      # RBD image format. Defaults to "2".
      imageFormat: "2"

      # RBD image features, equivalent to OR'd bitfield value: 63
      # Available for imageFormat: "2". Older releases of CSI RBD
      # support only the `layering` feature. The Linux kernel (KRBD) supports the
      # full feature complement as of 5.4
      imageFeatures: layering

      # These secrets contain Ceph admin credentials.
      csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
      csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
      csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
      # Specify the filesystem type of the volume. If not specified, csi-provisioner
      # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
      # in hyperconverged settings where the volume is mounted on the same node as the osds.
      csi.storage.k8s.io/fstype: ext4

cephFileSystem:
  enabled: true
  # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
  spec:
    metadataPool:
      replicated:
        size: ###ZARF_VAR_HA_REPLICAS###
    dataPools:
      - failureDomain: host
        replicated:
          size: ###ZARF_VAR_HA_REPLICAS###
        # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
        name: data0
    metadataServer:
      # Can be increased separately from the number of pools for HA
      activeCount: ###ZARF_VAR_ACTIVE_METADATA_SERVERS###
      activeStandby: true
      resources:
        limits:
          cpu: "2000m"
          memory: "4Gi"
        requests:
          cpu: "1000m"
          memory: "4Gi"
      priorityClassName: system-cluster-critical
  storageClass:
    isDefault: false
    # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
    pool: data0
    reclaimPolicy: Retain
    allowVolumeExpansion: true
    volumeBindingMode: "Immediate"
    mountOptions: []
    # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
    parameters:
      # The secrets contain Ceph admin credentials.
      csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
      csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
      csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
      csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
      csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
      # Specify the filesystem type of the volume. If not specified, csi-provisioner
      # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
      # in hyperconverged settings where the volume is mounted on the same node as the osds.
      csi.storage.k8s.io/fstype: ext4

cephObjectStore:
  enabled: true
  # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: ###ZARF_VAR_HA_REPLICAS###
    dataPool:
      failureDomain: host
      erasureCoded:
        dataChunks: 2
        codingChunks: 1
    preservePoolsOnDelete: true
    gateway:
      port: 80
      resources:
        limits:
          cpu: "2000m"
          memory: "2Gi"
        requests:
          cpu: "1000m"
          memory: "1Gi"
      # securePort: 443
      # sslCertificateRef:
      instances: ###ZARF_VAR_HA_REPLICAS###
      priorityClassName: system-cluster-critical
  storageClass:
    reclaimPolicy: Retain
    volumeBindingMode: "Immediate"
    # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
    parameters:
      region: ###ZARF_VAR_REGION###
